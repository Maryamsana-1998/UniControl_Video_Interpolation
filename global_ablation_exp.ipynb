{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f321b7f8-4313-46e3-b808-5d3c9bab0a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "UniControlNet: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /data/maryam.sana/anaconda3/envs/unicontrolwrap/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /data/maryam.sana/anaconda3/envs/unicontrolwrap/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.test.video_codec import process\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "def create_model(config_path):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    model = instantiate_from_config(config.model)\n",
    "    return model\n",
    "\n",
    "model = create_model('experiments/global_ablation/uni_v15.yaml').cpu()\n",
    "\n",
    "# 3. Load checkpoint\n",
    "ckpt = torch.load('experiments/global_ablation/uni_cap.ckpt', map_location='cuda')\n",
    "# model_keys = set(model.state_dict())\n",
    "# filtered = {k: v for k, v in ckpt['state_dict'].items() if k in model_keys}\n",
    "model.load_state_dict(ckpt, strict=False)\n",
    "model = model.cuda().eval()\n",
    "target_resolution = (512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d91ed22-00b9-4b49-a19e-6da0444e4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /data/maryam.sana/anaconda3/envs/unicontrolwrap/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected timesteps for ddim sampler: [  1  21  41  61  81 101 121 141 161 181 201 221 241 261 281 301 321 341\n",
      " 361 381 401 421 441 461 481 501 521 541 561 581 601 621 641 661 681 701\n",
      " 721 741 761 781 801 821 841 861 881 901 921 941 961 981]\n",
      "Selected alphas for ddim sampler: a_t: tensor([0.9983, 0.9804, 0.9609, 0.9398, 0.9171, 0.8930, 0.8674, 0.8404, 0.8121,\n",
      "        0.7827, 0.7521, 0.7207, 0.6885, 0.6557, 0.6224, 0.5888, 0.5551, 0.5215,\n",
      "        0.4882, 0.4552, 0.4229, 0.3913, 0.3605, 0.3308, 0.3023, 0.2750, 0.2490,\n",
      "        0.2245, 0.2014, 0.1799, 0.1598, 0.1413, 0.1243, 0.1087, 0.0946, 0.0819,\n",
      "        0.0705, 0.0604, 0.0514, 0.0435, 0.0365, 0.0305, 0.0254, 0.0210, 0.0172,\n",
      "        0.0140, 0.0113, 0.0091, 0.0073, 0.0058]); a_(t-1): [0.99914998 0.99829602 0.98038077 0.96087277 0.93978298 0.91713792\n",
      " 0.89298052 0.86737001 0.84038192 0.81210774 0.78265446 0.75214338\n",
      " 0.72070938 0.68849909 0.65566933 0.62238538 0.58881873 0.55514455\n",
      " 0.52153981 0.4881804  0.45523876 0.42288151 0.39126703 0.36054322\n",
      " 0.33084565 0.30229566 0.27499905 0.24904492 0.22450483 0.20143245\n",
      " 0.1798636  0.15981644 0.14129217 0.12427604 0.10873855 0.09463691\n",
      " 0.08191671 0.0705137  0.06035557 0.05136392 0.043456   0.03654652\n",
      " 0.03054927 0.02537862 0.02095082 0.01718517 0.0140049  0.01133791\n",
      " 0.00911731 0.00728173]\n",
      "For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], dtype=torch.float64)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████████████████████████████████████████████████| 50/50 [00:12<00:00,  4.16it/s]\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected timesteps for ddim sampler: [  1  21  41  61  81 101 121 141 161 181 201 221 241 261 281 301 321 341\n",
      " 361 381 401 421 441 461 481 501 521 541 561 581 601 621 641 661 681 701\n",
      " 721 741 761 781 801 821 841 861 881 901 921 941 961 981]\n",
      "Selected alphas for ddim sampler: a_t: tensor([0.9983, 0.9804, 0.9609, 0.9398, 0.9171, 0.8930, 0.8674, 0.8404, 0.8121,\n",
      "        0.7827, 0.7521, 0.7207, 0.6885, 0.6557, 0.6224, 0.5888, 0.5551, 0.5215,\n",
      "        0.4882, 0.4552, 0.4229, 0.3913, 0.3605, 0.3308, 0.3023, 0.2750, 0.2490,\n",
      "        0.2245, 0.2014, 0.1799, 0.1598, 0.1413, 0.1243, 0.1087, 0.0946, 0.0819,\n",
      "        0.0705, 0.0604, 0.0514, 0.0435, 0.0365, 0.0305, 0.0254, 0.0210, 0.0172,\n",
      "        0.0140, 0.0113, 0.0091, 0.0073, 0.0058]); a_(t-1): [0.99914998 0.99829602 0.98038077 0.96087277 0.93978298 0.91713792\n",
      " 0.89298052 0.86737001 0.84038192 0.81210774 0.78265446 0.75214338\n",
      " 0.72070938 0.68849909 0.65566933 0.62238538 0.58881873 0.55514455\n",
      " 0.52153981 0.4881804  0.45523876 0.42288151 0.39126703 0.36054322\n",
      " 0.33084565 0.30229566 0.27499905 0.24904492 0.22450483 0.20143245\n",
      " 0.1798636  0.15981644 0.14129217 0.12427604 0.10873855 0.09463691\n",
      " 0.08191671 0.0705137  0.06035557 0.05136392 0.043456   0.03654652\n",
      " 0.03054927 0.02537862 0.02095082 0.01718517 0.0140049  0.01133791\n",
      " 0.00911731 0.00728173]\n",
      "For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], dtype=torch.float64)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████████████████████████████████████████████████| 50/50 [00:12<00:00,  4.12it/s]\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected timesteps for ddim sampler: [  1  21  41  61  81 101 121 141 161 181 201 221 241 261 281 301 321 341\n",
      " 361 381 401 421 441 461 481 501 521 541 561 581 601 621 641 661 681 701\n",
      " 721 741 761 781 801 821 841 861 881 901 921 941 961 981]\n",
      "Selected alphas for ddim sampler: a_t: tensor([0.9983, 0.9804, 0.9609, 0.9398, 0.9171, 0.8930, 0.8674, 0.8404, 0.8121,\n",
      "        0.7827, 0.7521, 0.7207, 0.6885, 0.6557, 0.6224, 0.5888, 0.5551, 0.5215,\n",
      "        0.4882, 0.4552, 0.4229, 0.3913, 0.3605, 0.3308, 0.3023, 0.2750, 0.2490,\n",
      "        0.2245, 0.2014, 0.1799, 0.1598, 0.1413, 0.1243, 0.1087, 0.0946, 0.0819,\n",
      "        0.0705, 0.0604, 0.0514, 0.0435, 0.0365, 0.0305, 0.0254, 0.0210, 0.0172,\n",
      "        0.0140, 0.0113, 0.0091, 0.0073, 0.0058]); a_(t-1): [0.99914998 0.99829602 0.98038077 0.96087277 0.93978298 0.91713792\n",
      " 0.89298052 0.86737001 0.84038192 0.81210774 0.78265446 0.75214338\n",
      " 0.72070938 0.68849909 0.65566933 0.62238538 0.58881873 0.55514455\n",
      " 0.52153981 0.4881804  0.45523876 0.42288151 0.39126703 0.36054322\n",
      " 0.33084565 0.30229566 0.27499905 0.24904492 0.22450483 0.20143245\n",
      " 0.1798636  0.15981644 0.14129217 0.12427604 0.10873855 0.09463691\n",
      " 0.08191671 0.0705137  0.06035557 0.05136392 0.043456   0.03654652\n",
      " 0.03054927 0.02537862 0.02095082 0.01718517 0.0140049  0.01133791\n",
      " 0.00911731 0.00728173]\n",
      "For the chosen value of eta, which is 0.0, this results in the following sigma_t schedule for ddim sampler tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.], dtype=torch.float64)\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 50 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████████████████████████████████████████████████| 50/50 [00:11<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from lpips import LPIPS\n",
    "from pytorch_msssim import ms_ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from src.test.video_codec import process\n",
    "from ldm.util import instantiate_from_config\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Initialize metrics\n",
    "psnr_values = []\n",
    "ms_ssim_values = []\n",
    "lpips_values = []\n",
    "video_names = []\n",
    "\n",
    "# Initialize LPIPS model\n",
    "lpips_model = LPIPS(net='alex').cuda()\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x * 255\n",
    "])\n",
    "\n",
    "# List of video names\n",
    "videos = {    \"Beauty\": {\n",
    "        \"prompt\": \"A beautiful blonde girl smiling with pink lipstick with black background\",\n",
    "        \"path\": \"Beauty\"\n",
    "    },\n",
    "    \"Jockey\": {\n",
    "        \"prompt\": \"A man riding a brown horse, galloping through a green race track. The man is wearing a yellow and red shirt and also a yellow hat\",\n",
    "        \"path\": \"Jockey\"\n",
    "    },\n",
    "    \"Bosphorus\": {\n",
    "        \"prompt\": \"A man and a woman sitting together on a boat sailing in water. They are both wearing ties. There is also a red flag at end of boat\",\n",
    "        \"path\": \"Bosphorus\"\n",
    "    }}\n",
    "\n",
    "# Base directory\n",
    "base_dir = Path('../Ultra_Perceptual_Video_Compression/data/UVG')\n",
    "\n",
    "for vid in videos.keys():\n",
    "    try:\n",
    "        # Paths\n",
    "        original_image_path = base_dir / vid / 'images' / 'frame_0001.png'\n",
    "        ref_image_path = base_dir / vid / 'intra_frames' / 'decoded_q4' / 'decoded_frame_0000.png'\n",
    "        flow_image_path = base_dir / vid / 'optical_flow' / 'optical_flow_gop_8' / 'flow_0000_0001.png'\n",
    "\n",
    "        # Load images\n",
    "        target_resolution = (512, 512)\n",
    "        ref_img = cv2.imread(str(ref_image_path))\n",
    "        ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)\n",
    "        ref_img = cv2.resize(ref_img, target_resolution)\n",
    "\n",
    "        flow_img = cv2.imread(str(flow_image_path))\n",
    "        flow_img = cv2.cvtColor(flow_img, cv2.COLOR_BGR2RGB)\n",
    "        flow_img = cv2.resize(flow_img, target_resolution)\n",
    "\n",
    "        local_images = [flow_img, ref_img]\n",
    "\n",
    "        # Run inference\n",
    "        prompt = videos[vid]['prompt']\n",
    "        pred = process(model, local_images, prompt)\n",
    "\n",
    "        # Save predicted image\n",
    "        pred_save_path = f'experiments/global_ablation/{vid}.png'\n",
    "        os.makedirs(os.path.dirname(pred_save_path), exist_ok=True)\n",
    "        cv2.imwrite(pred_save_path, cv2.cvtColor(pred[0][0], cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        # # Prepare tensors\n",
    "        # original_image = Image.open(original_image_path).convert(\"RGB\")\n",
    "        # pred_image = Image.fromarray(pred[0].astype(np.uint8))\n",
    "\n",
    "        # original_tensor = transform(original_image).unsqueeze(0).to('cuda')\n",
    "        # pred_tensor = transform(pred_image).unsqueeze(0).to('cuda')\n",
    "\n",
    "        # # PSNR\n",
    "        # psnr_value = psnr(original_tensor.squeeze().cpu().numpy(), pred_tensor.squeeze().cpu().numpy(), data_range=255)\n",
    "        # if psnr_value > 1000:\n",
    "        #     continue\n",
    "        # psnr_values.append(psnr_value)\n",
    "\n",
    "        # # MS-SSIM\n",
    "        # ms_ssim_value = ms_ssim(original_tensor, pred_tensor, data_range=255, size_average=True).item()\n",
    "        # ms_ssim_values.append(ms_ssim_value)\n",
    "\n",
    "        # # LPIPS\n",
    "        # lpips_value = lpips_model(original_tensor / 255.0, pred_tensor / 255.0).item()\n",
    "        # lpips_values.append(lpips_value)\n",
    "\n",
    "        # video_names.append(vid)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {vid}: {e}\")\n",
    "        continue\n",
    "\n",
    "# # Create dataframe\n",
    "# df = pd.DataFrame({\n",
    "#     'Video': video_names,\n",
    "#     'PSNR': psnr_values,\n",
    "#     'MS-SSIM': ms_ssim_values,\n",
    "#     'LPIPS': lpips_values\n",
    "# })\n",
    "\n",
    "# # Save dataframe\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd09e61-bc86-414f-9214-0d66af3230aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
